# Linal_lab2

# Лабораторная работа: Логистическая регрессия

## Цель
Реализовать и обучить логистическую регрессию на синтетических данных. Выполнить нормализацию признаков, оценить точность модели и проанализировать матрицу ошибок.

---

## Используемые библиотеки
| Библиотека | Назначение                                      |
|------------|-------------------------------------------------|
| `numpy`    | Векторные/матричные вычисления, генерация данных |

---

## Алгоритм: Логистическая регрессия
Модель реализует **логистическую регрессию** с использованием сигмоидной функции активации и мини-пакетного градиентного спуска.

### Ключевые компоненты:
- **Инициализация весов**:  
  `self.w = np.random.randn(input_size) * 0.01`  
  `self.b = 0.0`
- **Сигмоидная функция**:  
  ```python
  def _sigmoid(self, z):
      return 1 / (1 + np.exp(-np.clip(z, -100, 100)))
  ```
- **Функция потерь (бинарная кросс-энтропия)**:
  ```python
  -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))
  ```
- **Обновление весов**:
  ```python
  dw = np.dot(X_batch.T, error) / len(y_batch)
  db = np.mean(error)
  self.w -= learning_rate * dw
  self.b -= learning_rate * db
  ```

### Параметры обучения:
- `epochs=10`: Количество эпох
- `learning_rate=0.1`: Скорость обучения
- `batch_size=10000`: Размер мини-батча

---

## Этапы работы

### 1. Генерация синтетических данных
- **Примеры**: 1,000,000
- **Признаки**: 30
- **Шум**: 20% (инверсия меток)
- **Формула генерации**:
  ```python
  z = np.dot(X, true_weights) + true_bias
  probabilities = 1 / (1 + np.exp(-z))
  y = (probabilities > 0.5).astype(int)
  ```

### 2. Предобработка данных
- **Нормализация**:  
  `(X - np.mean(X, axis=0)) / np.std(X, axis=0)`
- **Разделение данных**: 80% train / 20% test

### 3. Обучение модели
Параметры обучения:
- Размер батча: 10,000
- Количество эпох: 10
- Скорость обучения: 0.1

### 4. Оценка модели
- **Accuracy**: Доля правильных предсказаний
- **Матрица ошибок**:
  ```python
  [[TN  FP]
   [FN  TP]]
  ```

---

## Результаты выполнения

### Генерация данных
```
Генерация данных...
Данные сгенерированы за 0.60s
Train shape: (800000, 30), Test shape: (200000, 30)
Class balance: 51.8% positive
```

### Процесс обучения
```
Обучение модели...
Epoch 0, Loss: 0.5711, Time: 0.02s
Epoch 1, Loss: 0.5641, Time: 0.02s
Epoch 2, Loss: 0.5635, Time: 0.02s
Epoch 3, Loss: 0.5634, Time: 0.02s
...
Epoch 9, Loss: 0.5634, Time: 0.02s
```

### Оценка качества
```
Test Accuracy: 79.79%
Confusion Matrix:
[[75131 21487]
 [18926 84456]]
```

---

## Анализ результатов
1. **Сходимость модели**:  
   Потери стабилизируются после 3-й эпохи (Loss ≈ 0.5634)
2. **Точность**: 79.79% на тестовой выборке
3. **Распределение ошибок**:
   - True Negative (TN): 75,131
   - False Positive (FP): 21,487
   - False Negative (FN): 18,926
   - True Positive (TP): 84,456
4. **Эффективность**:  
   Время обработки 1 эпохи ≈ 0.02 секунды

---

## Выводы
1. Модель корректно реализует логистическую регрессию с мини-пакетным градиентным спуском
2. Нормализация данных обеспечивает стабильность обучения
3. Достигнута удовлетворительная точность (79.8%) несмотря на высокий уровень шума (20%)
4. Наибольший вклад в ошибки вносят False Positive случаи
